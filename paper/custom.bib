@inproceedings{nakano-etal-2003-towards,
    title = "Towards a Model of Face-to-Face Grounding",
    author = "Nakano, Yukiko  and
      Reinstein, Gabe  and
      Stocky, Tom  and
      Cassell, Justine",
    booktitle = "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2003",
    address = "Sapporo, Japan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P03-1070/",
    doi = "10.3115/1075096.1075166",
    pages = "553--561"
}

@MISC{ai-paper-crawl,
  title        = "Seed42Lab/AI-paper-crawl",
  author       = {{Forty-Two AI Lab}},
  publisher    = "HuggingFace",
  howpublished = "\url{https://huggingface.co/datasets/Seed42Lab/AI-paper-crawl}",
}

@inproceedings{li-etal-2024-groundinggpt,
    title = "{G}rounding{GPT}: Language Enhanced Multi-modal Grounding Model",
    author = "Li, Zhaowei  and
      Xu, Qi  and
      Zhang, Dong  and
      Song, Hang  and
      Cai, YiQing  and
      Qi, Qi  and
      Zhou, Ran  and
      Pan, Junting  and
      Li, Zefeng  and
      Tu, Vu  and
      Huang, Zhida  and
      Wang, Tao",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.360/",
    doi = "10.18653/v1/2024.acl-long.360",
    pages = "6657--6678",
    abstract = "Multi-modal large language models (MLLMs) have demonstrated remarkable performance across various tasks. However, these models often prioritize capturing global information and overlook the importance of perceiving local information. This limitation hinders their ability to effectively understand fine-grained details and handle grounding tasks that necessitate nuanced comprehension. Although some recent works have made strides in this, they have primarily focused on single-modality inputs. Therefore, we propose \textbf{GroundingGPT}, an end-to-end language enhanced multi-modal grounding model. It is designed to perform fine-grained grounding tasks for three modalities: image, video and audio. To enhance the model`s performance, we adopt a coarse-to-fine training strategy, utilizing a three-stage training approach to progressively enhance the model`s semantic awareness and fine-grained understanding capabilities. Additionally, we employ a diversified stage-specific dataset construction pipeline, developing a multi-modal, multi-granularity dataset tailored for training the model in different stages. Extensive experiments conducted on multiple multi-modal benchmarks demonstrate that our model achieves impressive fine-grained understanding of multi-modal inputs on grounding tasks while maintaining or improving its global comprehension capabilities. Our code, model, and dataset are available at https://github.com/lzw-lzw/GroundingGPT."
}

@misc{xiao2024visualgroundingsurvey,
    title={Towards Visual Grounding: A Survey},
    author={Linhui Xiao and Xiaoshan Yang and Xiangyuan Lan and Yaowei Wang and Changsheng Xu},
    year={2024},
    eprint={2412.20206},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2412.20206}, 
}

@InProceedings{gigapixel,
    author    = {Ma, Tao and Bai, Bing and Lin, Haozhe and Wang, Heyuan and Wang, Yu and Luo, Lin and Fang, Lu},
    title     = {When Visual Grounding Meets Gigapixel-level Large-scale Scenes: Benchmark and Approach},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {22119-22128}
}

@InProceedings{Zeng_2024_CVPR_Comp_Challenge,
    author    = {Zeng, Yunan and Huang, Yan and Zhang, Jinjin and Jie, Zequn and Chai, Zhenhua and Wang, Liang},
    title     = {Investigating Compositional Challenges in Vision-Language Models for Visual Grounding},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {14141-14151}
}

@InProceedings{Zhou_2023_CVPR_Tracking,
    author    = {Zhou, Li and Zhou, Zikun and Mao, Kaige and He, Zhenyu},
    title     = {Joint Visual Grounding and Tracking With Natural Language Specification},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {23151-23160}
}

@inproceedings{neurips_zhang_contrastive_learning,
 author = {Zhang, Zhu and Zhao, Zhou and Lin, Zhijie and zhu, jieming and He, Xiuqiang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {18123--18134},
 publisher = {Curran Associates, Inc.},
 title = {Counterfactual Contrastive Learning for Weakly-Supervised Vision-Language Grounding},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/d27b95cac4c27feb850aaa4070cc4675-Paper.pdf},
 volume = {33},
 year = {2020}
}