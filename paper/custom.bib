@inproceedings{nakano-etal-2003-towards,
    title = "Towards a Model of Face-to-Face Grounding",
    author = "Nakano, Yukiko  and
      Reinstein, Gabe  and
      Stocky, Tom  and
      Cassell, Justine",
    booktitle = "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2003",
    address = "Sapporo, Japan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P03-1070/",
    doi = "10.3115/1075096.1075166",
    pages = "553--561"
}

@MISC{ai-paper-crawl,
  title        = "Seed42Lab/AI-paper-crawl",
  author       = {{Forty-Two AI Lab}},
  publisher    = "HuggingFace",
  howpublished = "\url{https://huggingface.co/datasets/Seed42Lab/AI-paper-crawl}",
}

@inproceedings{li-etal-2024-groundinggpt,
    title = "{G}rounding{GPT}: Language Enhanced Multi-modal Grounding Model",
    author = "Li, Zhaowei  and
      Xu, Qi  and
      Zhang, Dong  and
      Song, Hang  and
      Cai, YiQing  and
      Qi, Qi  and
      Zhou, Ran  and
      Pan, Junting  and
      Li, Zefeng  and
      Tu, Vu  and
      Huang, Zhida  and
      Wang, Tao",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.360/",
    doi = "10.18653/v1/2024.acl-long.360",
    pages = "6657--6678",
    abstract = "Multi-modal large language models (MLLMs) have demonstrated remarkable performance across various tasks. However, these models often prioritize capturing global information and overlook the importance of perceiving local information. This limitation hinders their ability to effectively understand fine-grained details and handle grounding tasks that necessitate nuanced comprehension. Although some recent works have made strides in this, they have primarily focused on single-modality inputs. Therefore, we propose \textbf{GroundingGPT}, an end-to-end language enhanced multi-modal grounding model. It is designed to perform fine-grained grounding tasks for three modalities: image, video and audio. To enhance the model`s performance, we adopt a coarse-to-fine training strategy, utilizing a three-stage training approach to progressively enhance the model`s semantic awareness and fine-grained understanding capabilities. Additionally, we employ a diversified stage-specific dataset construction pipeline, developing a multi-modal, multi-granularity dataset tailored for training the model in different stages. Extensive experiments conducted on multiple multi-modal benchmarks demonstrate that our model achieves impressive fine-grained understanding of multi-modal inputs on grounding tasks while maintaining or improving its global comprehension capabilities. Our code, model, and dataset are available at https://github.com/lzw-lzw/GroundingGPT."
}

@misc{xiao2024visualgroundingsurvey,
    title={Towards Visual Grounding: A Survey},
    author={Linhui Xiao and Xiaoshan Yang and Xiangyuan Lan and Yaowei Wang and Changsheng Xu},
    year={2024},
    eprint={2412.20206},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2412.20206}, 
}

@InProceedings{gigapixel,
    author    = {Ma, Tao and Bai, Bing and Lin, Haozhe and Wang, Heyuan and Wang, Yu and Luo, Lin and Fang, Lu},
    title     = {When Visual Grounding Meets Gigapixel-level Large-scale Scenes: Benchmark and Approach},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {22119-22128}
}

@InProceedings{Zeng_2024_CVPR_Comp_Challenge,
    author    = {Zeng, Yunan and Huang, Yan and Zhang, Jinjin and Jie, Zequn and Chai, Zhenhua and Wang, Liang},
    title     = {Investigating Compositional Challenges in Vision-Language Models for Visual Grounding},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {14141-14151}
}

@InProceedings{Zhou_2023_CVPR_Tracking,
    author    = {Zhou, Li and Zhou, Zikun and Mao, Kaige and He, Zhenyu},
    title     = {Joint Visual Grounding and Tracking With Natural Language Specification},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {23151-23160}
}

@inproceedings{neurips_zhang_contrastive_learning,
 author = {Zhang, Zhu and Zhao, Zhou and Lin, Zhijie and zhu, jieming and He, Xiuqiang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {18123--18134},
 publisher = {Curran Associates, Inc.},
 title = {Counterfactual Contrastive Learning for Weakly-Supervised Vision-Language Grounding},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/d27b95cac4c27feb850aaa4070cc4675-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{li-etal-2022-end-to-end-information-tree,
    title = "End-to-End Modeling via Information Tree for One-Shot Natural Language Spatial Video Grounding",
    author = "Li, Mengze  and
      Wang, Tianbao  and
      Zhang, Haoyu  and
      Zhang, Shengyu  and
      Zhao, Zhou  and
      Miao, Jiaxu  and
      Zhang, Wenqiao  and
      Tan, Wenming  and
      Wang, Jin  and
      Wang, Peng  and
      Pu, Shiliang  and
      Wu, Fei",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.596/",
    doi = "10.18653/v1/2022.acl-long.596",
    pages = "8707--8717",
    abstract = "Natural language spatial video grounding aims to detect the relevant objects in video frames with descriptive sentences as the query. In spite of the great advances, most existing methods rely on dense video frame annotations, which require a tremendous amount of human effort. To achieve effective grounding under a limited annotation budget, we investigate one-shot video grounding and learn to ground natural language in all video frames with solely one frame labeled, in an end-to-end manner. One major challenge of end-to-end one-shot video grounding is the existence of videos frames that are either irrelevant to the language query or the labeled frame. Another challenge relates to the limited supervision, which might result in ineffective representation learning. To address these challenges, we designed an end-to-end model via Information Tree for One-Shot video grounding (IT-OS). Its key module, the information tree, can eliminate the interference of irrelevant frames based on branch search and branch cropping techniques. In addition, several self-supervised tasks are proposed based on the information tree to improve the representation learning under insufficient labeling. Experiments on the benchmark dataset demonstrate the effectiveness of our model."
}

@inproceedings{cheng-etal-2024-seeclick,
    title = "{S}ee{C}lick: Harnessing {GUI} Grounding for Advanced Visual {GUI} Agents",
    author = "Cheng, Kanzhi  and
      Sun, Qiushi  and
      Chu, Yougang  and
      Xu, Fangzhi  and
      YanTao, Li  and
      Zhang, Jianbing  and
      Wu, Zhiyong",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.505/",
    doi = "10.18653/v1/2024.acl-long.505",
    pages = "9313--9332",
    abstract = "Graphical User Interface (GUI) agents are designed to automate complex tasks on digital devices, such as smartphones and desktops. Most existing GUI agents interact with the environment through extracted structured data, which can be notably lengthy (e.g., HTML) and occasionally inaccessible (e.g., on desktops). To alleviate this issue, we propose a novel visual GUI agent {--} SeeClick, which only relies on screenshots for task automation. In our preliminary study, we have discovered a key challenge in developing visual GUI agents: GUI grounding {--} the capacity to accurately locate screen elements based on instructions. To tackle this challenge, we propose to enhance SeeClick with GUI grounding pre-training and devise a method to automate the curation of GUI grounding data. Along with the efforts above, we have also created ScreenSpot, the first realistic GUI grounding benchmark that encompasses mobile, desktop, and web environments. After pre-training, SeeClick demonstrates significant improvement in ScreenSpot over various baselines. Moreover, comprehensive evaluations on three widely used benchmarks consistently support our finding that advancements in GUI grounding directly correlate with enhanced performance in downstream GUI agent tasks. The model, data and code will be open-sourced."
}

@InProceedings{Wasim_2024_CVPR_DINO,
    author    = {Wasim, Syed Talal and Naseer, Muzammal and Khan, Salman and Yang, Ming-Hsuan and Khan, Fahad Shahbaz},
    title     = {VideoGrounding-DINO: Towards Open-Vocabulary Spatio-Temporal Video Grounding},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {18909-18918}
}

@misc{chen2022groundinganswersvisualquestions,
      title={Grounding Answers for Visual Questions Asked by Visually Impaired People}, 
      author={Chongyan Chen and Samreen Anjum and Danna Gurari},
      year={2022},
      eprint={2202.01993},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2202.01993}, 
}

@InProceedings{Tian_2021_CVPR_cyclic_audio,
    author    = {Tian, Yapeng and Hu, Di and Xu, Chenliang},
    title     = {Cyclic Co-Learning of Sounding Object Visual Grounding and Sound Separation},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {2745-2754}
}

@InProceedings{Cai_2022_CVPR_3DJCG,
    author    = {Cai, Daigang and Zhao, Lichen and Zhang, Jing and Sheng, Lu and Xu, Dong},
    title     = {3DJCG: A Unified Framework for Joint Dense Captioning and Visual Grounding on 3D Point Clouds},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {16464-16473}
}

@InProceedings{Chen_2024_CVPR_self_supervised_spatio_temporal,
    author    = {Chen, Brian and Shvetsova, Nina and Rouditchenko, Andrew and Kondermann, Daniel and Thomas, Samuel and Chang, Shih-Fu and Feris, Rogerio and Glass, James and Kuehne, Hilde},
    title     = {What When and Where? Self-Supervised Spatio-Temporal Grounding in Untrimmed Multi-Action Videos from Narrated Instructions},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {18419-18429}
}

@InProceedings{Yang_2021_ICCV_sat_2d,
    author    = {Yang, Zhengyuan and Zhang, Songyang and Wang, Liwei and Luo, Jiebo},
    title     = {SAT: 2D Semantics Assisted Training for 3D Visual Grounding},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {1856-1866}
}

@InProceedings{Chen_2021_ICCV_video_entailment,
    author    = {Chen, Junwen and Kong, Yu},
    title     = {Explainable Video Entailment With Grounded Visual Evidence},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {2021-2030}
}

@InProceedings{Chen_2023_ICCV_vqa,
    author    = {Chen, Chongyan and Anjum, Samreen and Gurari, Danna},
    title     = {VQA Therapy: Exploring Answer Differences by Visually Grounding Answers},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {15315-15325}
}

@misc{guan2022neurofluidfluiddynamicsgrounding,
      title={NeuroFluid: Fluid Dynamics Grounding with Particle-Driven Neural Radiance Fields}, 
      author={Shanyan Guan and Huayu Deng and Yunbo Wang and Xiaokang Yang},
      year={2022},
      eprint={2203.01762},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2203.01762}, 
}

@inproceedings{NEURIPS2024_neuma_material_visual_grounding,
 author = {Cao, Junyi and Guan, Shanyan and Ge, Yanhao and Li, Wei and Yang, Xiaokang and Ma, Chao},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {65643--65669},
 publisher = {Curran Associates, Inc.},
 title = {NeuMA: Neural Material Adaptor for Visual Grounding of Intrinsic Dynamics},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/78b6beab44f92adc74ac1fdb212ac3a0-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}

@misc{zheng2024gpt4visiongeneralistwebagent,
      title={GPT-4V(ision) is a Generalist Web Agent, if Grounded}, 
      author={Boyuan Zheng and Boyu Gou and Jihyung Kil and Huan Sun and Yu Su},
      year={2024},
      eprint={2401.01614},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2401.01614}, 
}

@inproceedings{NIPS2014_markov_logic_networks,
 author = {Venugopal, Deepak and Gogate, Vibhav},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Scaling-up Importance Sampling for Markov Logic Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/4274adea26e813dd9ea92f9b2ec0eb2e-Paper.pdf},
 volume = {27},
 year = {2014}
}

@inproceedings{MLN4KB_markov,
  author = {Fang, Huang and Liu, Yang and Cai, Yunfeng and Sun, Mingming},
  title = {MLN4KB: an efficient Markov logic network engine for large-scale knowledge bases and structured logic rules},
  year = {2023},
  isbn = {9781450394161},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3543507.3583248},
  doi = {10.1145/3543507.3583248},
  abstract = {Markov logic network (MLN) is a powerful statistical modeling framework for probabilistic logic reasoning. Despite the elegancy and effectiveness of MLN, the inference of MLN is known to suffer from an efficiency issue. Even the state-of-the-art MLN engines can not scale to medium-size real-world knowledge bases in the open-world setting, i.e., all unobserved facts in the knowledge base need predictions. In this work, by focusing on a certain class of first-order logic rules that are sufficiently expressive, we develop a highly efficient MLN inference engine called MLN4KB that can leverage the sparsity of knowledge bases. MLN4KB enjoys quite strong theoretical properties; its space and time complexities can be exponentially smaller than existing MLN engines. Experiments on both synthetic and real-world knowledge bases demonstrate the effectiveness of the proposed method. MLN4KB is orders of magnitudes faster (more than 103 times faster on some datasets) than existing MLN engines in the open-world setting. Without any approximation tricks, MLN4KB can scale to real-world knowledge bases including WN-18 and YAGO3-10 and achieve decent prediction accuracy without bells and whistles. We implement MLN4KB as a Julia package called MLN4KB.jl. The package supports both maximum a posteriori (MAP) inference and learning the weights of rules. MLN4KB.jl is public available at https://github.com/baidu-research/MLN4KB .},
  booktitle = {Proceedings of the ACM Web Conference 2023},
  pages = {2423–2432},
  numpages = {10},
  keywords = {Markov logic network, knowledge graph completion.},
  location = {Austin, TX, USA},
  series = {WWW '23}
}

@misc{jin2022embracingconsistencyonestageapproach,
      title={Embracing Consistency: A One-Stage Approach for Spatio-Temporal Video Grounding}, 
      author={Yang Jin and Yongzhi Li and Zehuan Yuan and Yadong Mu},
      year={2022},
      eprint={2209.13306},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2209.13306}, 
}

@inproceedings{NEURIPS2023_exploiting_context_3d,
 author = {Yang, Li and yuan, chunfeng and Zhang, Ziqi and Qi, Zhongang and Xu, Yan and Liu, Wei and Shan, Ying and Li, Bing and Yang, Weiping and Li, Peng and Wang, Yan and Hu, Weiming},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {49542--49554},
 publisher = {Curran Associates, Inc.},
 title = {Exploiting Contextual Objects and Relations for 3D Visual Grounding},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/9b91ee0da3bcd61905fcd89e770168fc-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@inproceedings{NEURIPS2023_how_to_video,
 author = {Afouras, Triantafyllos and Mavroudi, Effrosyni and Nagarajan, Tushar and Wang, Huiyu and Torresani, Lorenzo},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {50310--50326},
 publisher = {Curran Associates, Inc.},
 title = {HT-Step: Aligning Instructional Articles with How-To Videos},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/9d58d85bfc041b4f901c62ba37a3f322-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}

@inproceedings{NEURIPS2023_city_refer_3d,
 author = {Miyanishi, Taiki and Kitamori, Fumiya and Kurita, Shuhei and Lee, Jungdae and Kawanabe, Motoaki and Inoue, Nakamasa},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {77758--77770},
 publisher = {Curran Associates, Inc.},
 title = {CityRefer: Geography-aware 3D Visual Grounding Dataset on  City-scale Point Cloud Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/f4cef76305dcad4efd3537da087ff520-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}

@misc{ma2020learninggenerategroundedvisual,
      title={Learning to Generate Grounded Visual Captions without Localization Supervision}, 
      author={Chih-Yao Ma and Yannis Kalantidis and Ghassan AlRegib and Peter Vajda and Marcus Rohrbach and Zsolt Kira},
      year={2020},
      eprint={1906.00283},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1906.00283}, 
}

@misc{zhang2023llavagroundinggroundedvisualchat,
      title={LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models}, 
      author={Hao Zhang and Hongyang Li and Feng Li and Tianhe Ren and Xueyan Zou and Shilong Liu and Shijia Huang and Jianfeng Gao and Lei Zhang and Chunyuan Li and Jianwei Yang},
      year={2023},
      eprint={2312.02949},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2312.02949}, 
}

@misc{zhu2024scanreasonempowering3dvisual,
      title={ScanReason: Empowering 3D Visual Grounding with Reasoning Capabilities}, 
      author={Chenming Zhu and Tai Wang and Wenwei Zhang and Kai Chen and Xihui Liu},
      year={2024},
      eprint={2407.01525},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2407.01525}, 
}

@misc{lee2024regroundimprovingtextualspatial,
      title={ReGround: Improving Textual and Spatial Grounding at No Cost}, 
      author={Phillip Y. Lee and Minhyuk Sung},
      year={2024},
      eprint={2403.13589},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2403.13589}, 
}

@inproceedings{Roque2007ReactingTA_dialogue,
  title={Reacting to Agreement and Error in Spoken Dialogue Systems Using Degrees of Groundedness},
  author={Antonio Roque},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2007},
  url={https://api.semanticscholar.org/CorpusID:12846913}
}

@article{Tellex_Kollar_Dickerson_Walter_Banerjee_Teller_Roy_2011_robotic_navigation,
  title={Understanding Natural Language Commands for Robotic Navigation and Mobile Manipulation},
  volume={25},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/7979},
  DOI={10.1609/aaai.v25i1.7979},
  abstractNote={ &lt;p&gt; This paper describes a new model for understanding natural language commands given to autonomous systems that perform navigation and mobile manipulation in semi-structured environments. Previous approaches have used models with fixed structure to infer the likelihood of a sequence of actions given the environment and the command. In contrast, our framework, called Generalized Grounding Graphs, dynamically instantiates a probabilistic graphical model for a particular natural language command according to the command’s hierarchical and compositional semantic structure. Our system performs inference in the model to successfully find and execute plans corresponding to natural language commands such as &quot;Put the tire pallet on the truck.&quot; The model is trained using a corpus of commands collected using crowdsourcing. We pair each command with robot actions and use the corpus to learn the parameters of the model. We evaluate the robot’s performance by inferring plans from natural language commands, executing each plan in a realistic robot simulator, and asking users to evaluate the system’s performance. We demonstrate that our system can successfully follow many natural language commands from the corpus. &lt;/p&gt; },
  number={1},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  author={Tellex, Stefanie and Kollar, Thomas and Dickerson, Steven and Walter, Matthew and Banerjee, Ashis and Teller, Seth and Roy, Nicholas},
  year={2011},
  month={Aug.},
  pages={1507-1514}
}

@inproceedings{liu-etal-2012-towards,
    title = "Towards Mediating Shared Perceptual Basis in Situated Dialogue",
    author = "Liu, Changsong  and
      Fang, Rui  and
      Chai, Joyce",
    editor = "Lee, Gary Geunbae  and
      Ginzburg, Jonathan  and
      Gardent, Claire  and
      Stent, Amanda",
    booktitle = "Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue",
    month = jul,
    year = "2012",
    address = "Seoul, South Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W12-1621/",
    pages = "140--149"
}

@article{Bao_Zheng_Mu_2021_dense_events, title={Dense Events Grounding in Video}, volume={35}, url={https://ojs.aaai.org/index.php/AAAI/article/view/16175}, DOI={10.1609/aaai.v35i2.16175}, abstractNote={This paper explores a novel setting of temporal sentence grounding for the first time, dubbed as dense events grounding. Given an untrimmed video and a paragraph description, dense events grounding aims to jointly localize temporal moments of multiple events described in the paragraph. Our main motivating fact is that multiple events to be grounded in a video are often semantically related and temporally coordinated according to their order appearing in the paragraph. This fact sheds light on devising more accurate visual grounding model. In this work, we propose Dense Events Propagation Network (DepNet) for this novel task. DepNet first adaptively aggregates temporal and semantic information of dense events into a compact set through a second-order attention pooling, then selectively propagates the aggregated information to each single event with soft attention. Based on such aggregation-and-propagation mechanism, DepNet can effectively exploit both the temporal order and semantic relations of dense events. We conduct comprehensive experiments on large-scale datasets ActivityNet Captions and TACoS. For fair comparisons, our evaluations include both state-of-art single-event grounding methods and their natural extensions to the dense-events grounding setting implemented by us. All experiments clearly shows the performance superiority of the proposed DepNet by significant margins.}, number={2}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Bao, Peijun and Zheng, Qian and Mu, Yadong}, year={2021}, month={May}, pages={920-928} }

@misc{huang2021deconfoundedvisualgrounding,
      title={Deconfounded Visual Grounding}, 
      author={Jianqiang Huang and Yu Qin and Jiaxin Qi and Qianru Sun and Hanwang Zhang},
      year={2021},
      eprint={2112.15324},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2112.15324}, 
}

@article{Islam_Gladstone_Iqbal_2023_PATRON, title={PATRON: Perspective-Aware Multitask Model for Referring Expression Grounding Using Embodied Multimodal Cues}, volume={37}, url={https://ojs.aaai.org/index.php/AAAI/article/view/25177}, DOI={10.1609/aaai.v37i1.25177}, abstractNote={Humans naturally use referring expressions with verbal utterances and nonverbal gestures to refer to objects and events. As these referring expressions can be interpreted differently from the speaker’s or the observer’s perspective, people effectively decide on the perspective in comprehending the expressions. However, existing models do not explicitly learn perspective grounding, which often causes the models to perform poorly in understanding embodied referring expressions. To make it exacerbate, these models are often trained on datasets collected in non-embodied settings without nonverbal gestures and curated from an exocentric perspective. To address these issues, in this paper, we present a perspective-aware multitask learning model, called PATRON, for relation and object grounding tasks in embodied settings by utilizing verbal utterances and nonverbal cues. In PATRON, we have developed a guided fusion approach, where a perspective grounding task guides the relation and object grounding task. Through this approach, PATRON learns disentangled task-specific and task-guidance representations, where task-guidance representations guide the extraction of salient multimodal features to ground the relation and object accurately. Furthermore, we have curated a synthetic dataset of embodied referring expressions with multimodal cues, called CAESAR-PRO. The experimental results suggest that PATRON outperforms the evaluated state-of-the-art visual-language models. Additionally, the results indicate that learning to ground perspective helps machine learning models to improve the performance of the relation and object grounding task. Furthermore, the insights from the extensive experimental results and the proposed dataset will enable researchers to evaluate visual-language models’ effectiveness in understanding referring expressions in other embodied settings.}, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Islam, Md Mofijul and Gladstone, Alexi and Iqbal, Tariq}, year={2023}, month={Jun.}, pages={971-979} }

@misc{wang2023cycleconsistencylearningcaptioninggrounding,
      title={Cycle-Consistency Learning for Captioning and Grounding}, 
      author={Ning Wang and Jiajun Deng and Mingbo Jia},
      year={2023},
      eprint={2312.15162},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2312.15162}, 
}

@article{Jiang_Cheng_Liu_Fang_Peng_Liu_2024_comprehensive_visual_grounding, title={Comprehensive Visual Grounding for Video Description}, volume={38}, url={https://ojs.aaai.org/index.php/AAAI/article/view/28032}, DOI={10.1609/aaai.v38i3.28032}, abstractNote={The grounding accuracy of existing video captioners is still behind the expectation. The majority of existing methods perform grounded video captioning on sparse entity annotations, whereas the captioning accuracy often suffers from degenerated object appearances on the annotated area such as motion blur and video defocus. Moreover, these methods seldom consider the complex interactions among entities. In this paper, we propose a comprehensive visual grounding network to improve video captioning, by explicitly linking the entities and actions to the visual clues across the video frames. Specifically, the network consists of spatial-temporal entity grounding and action grounding. The proposed entity grounding encourages the attention mechanism to focus on informative spatial areas across video frames, albeit the entity is annotated in only one frame of a video. The action grounding dynamically associates the verbs to related subjects and the corresponding context, which keeps fine-grained spatial and temporal details for action prediction. Both entity grounding and action grounding are formulated as a unified task guided by a soft grounding supervision, which brings architecture simplification and improves training efficiency as well. We conduct extensive experiments on two challenging datasets, and demonstrate significant performance improvements of +2.3 CIDEr on ActivityNet-Entities and +2.2 CIDEr on MSR-VTT compared to state-of-the-arts.}, number={3}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Jiang, Wenhui and Cheng, Yibo and Liu, Linxin and Fang, Yuming and Peng, Yuxin and Liu, Yang}, year={2024}, month={Mar.}, pages={2552-2560} }

@misc{shaikh2024groundinggapslanguagemodel,
      title={Grounding Gaps in Language Model Generations}, 
      author={Omar Shaikh and Kristina Gligorić and Ashna Khetan and Matthias Gerstgrasser and Diyi Yang and Dan Jurafsky},
      year={2024},
      eprint={2311.09144},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.09144}, 
}

@misc{wang2023programmaticallygroundedcompositionallygeneralizable,
      title={Programmatically Grounded, Compositionally Generalizable Robotic Manipulation}, 
      author={Renhao Wang and Jiayuan Mao and Joy Hsu and Hang Zhao and Jiajun Wu and Yang Gao},
      year={2023},
      eprint={2304.13826},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2304.13826}, 
}

@misc{peng2023kosmos2groundingmultimodallarge,
      title={Kosmos-2: Grounding Multimodal Large Language Models to the World}, 
      author={Zhiliang Peng and Wenhui Wang and Li Dong and Yaru Hao and Shaohan Huang and Shuming Ma and Furu Wei},
      year={2023},
      eprint={2306.14824},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.14824}, 
}

@inproceedings{kameko-etal-2015-symbol,
    title = "Can Symbol Grounding Improve Low-Level {NLP}? Word Segmentation as a Case Study",
    author = "Kameko, Hirotaka  and
      Mori, Shinsuke  and
      Tsuruoka, Yoshimasa",
    editor = "M{\`a}rquez, Llu{\'i}s  and
      Callison-Burch, Chris  and
      Su, Jian",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1277/",
    doi = "10.18653/v1/D15-1277",
    pages = "2298--2303"
}

@misc{jeong2024groundavideozeroshotgroundedvideo,
      title={Ground-A-Video: Zero-shot Grounded Video Editing using Text-to-image Diffusion Models}, 
      author={Hyeonho Jeong and Jong Chul Ye},
      year={2024},
      eprint={2310.01107},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2310.01107}, 
}

@inproceedings{chen-etal-2018-temporally,
    title = "Temporally Grounding Natural Sentence in Video",
    author = "Chen, Jingyuan  and
      Chen, Xinpeng  and
      Ma, Lin  and
      Jie, Zequn  and
      Chua, Tat-Seng",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1015/",
    doi = "10.18653/v1/D18-1015",
    pages = "162--171",
    abstract = "We introduce an effective and efficient method that grounds (i.e., localizes) natural sentences in long, untrimmed video sequences. Specifically, a novel Temporal GroundNet (TGN) is proposed to temporally capture the evolving fine-grained frame-by-word interactions between video and sentence. TGN sequentially scores a set of temporal candidates ended at each frame based on the exploited frame-by-word interactions, and finally grounds the segment corresponding to the sentence. Unlike traditional methods treating the overlapping segments separately in a sliding window fashion, TGN aggregates the historical information and generates the final grounding result in one single pass. We extensively evaluate our proposed TGN on three public datasets with significant improvements over the state-of-the-arts. We further show the consistent effectiveness and efficiency of TGN through an ablation study and a runtime test."
}

@inproceedings{jiang-etal-2019-tiger,
    title = "{TIGE}r: Text-to-Image Grounding for Image Caption Evaluation",
    author = "Jiang, Ming  and
      Huang, Qiuyuan  and
      Zhang, Lei  and
      Wang, Xin  and
      Zhang, Pengchuan  and
      Gan, Zhe  and
      Diesner, Jana  and
      Gao, Jianfeng",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1220/",
    doi = "10.18653/v1/D19-1220",
    pages = "2141--2152",
    abstract = "This paper presents a new metric called TIGEr for the automatic evaluation of image captioning systems. Popular metrics, such as BLEU and CIDEr, are based solely on text matching between reference captions and machine-generated captions, potentially leading to biased evaluations because references may not fully cover the image content and natural language is inherently ambiguous. Building upon a machine-learned text-image grounding model, TIGEr allows to evaluate caption quality not only based on how well a caption represents image content, but also on how well machine-generated captions match human-generated captions. Our empirical tests show that TIGEr has a higher consistency with human judgments than alternative existing metrics. We also comprehensively assess the metric`s effectiveness in caption evaluation by measuring the correlation between human judgments and metric scores."
}

@misc{wang20233drpnet3drelativepositionaware,
      title={3DRP-Net: 3D Relative Position-aware Network for 3D Visual Grounding}, 
      author={Zehan Wang and Haifeng Huang and Yang Zhao and Linjun Li and Xize Cheng and Yichen Zhu and Aoxiong Yin and Zhou Zhao},
      year={2023},
      eprint={2307.13363},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2307.13363}, 
}

@inproceedings{lu-etal-2022-extending,
    title = "Extending Phrase Grounding with Pronouns in Visual Dialogues",
    author = "Lu, Panzhong  and
      Zhang, Xin  and
      Zhang, Meishan  and
      Zhang, Min",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.518/",
    doi = "10.18653/v1/2022.emnlp-main.518",
    pages = "7614--7625",
    abstract = "Conventional phrase grounding aims to localize noun phrases mentioned in a given caption to their corresponding image regions, which has achieved great success recently. Apparently, sole noun phrase grounding is not enough for cross-modal visual language understanding. Here we extend the task by considering pronouns as well. First, we construct a dataset of phrase grounding with both noun phrases and pronouns to image regions. Based on the dataset, we test the performance of phrase grounding by using a state-of-the-art literature model of this line. Then, we enhance the baseline grounding model with coreference information which should help our task potentially, modeling the coreference structures with graph convolutional networks. Experiments on our dataset, interestingly, show that pronouns are easier to ground than noun phrases, where the possible reason might be that these pronouns are much less ambiguous. Additionally, our final model with coreference information can significantly boost the grounding performance of both noun phrases and pronouns."
}

@inproceedings{dou-peng-2021-improving,
    title = "Improving Pre-trained Vision-and-Language Embeddings for Phrase Grounding",
    author = "Dou, Zi-Yi  and
      Peng, Nanyun",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.513/",
    doi = "10.18653/v1/2021.emnlp-main.513",
    pages = "6362--6371",
    abstract = "Phrase grounding aims to map textual phrases to their associated image regions, which can be a prerequisite for multimodal reasoning and can benefit tasks requiring identifying objects based on language. With pre-trained vision-and-language models achieving impressive performance across tasks, it remains unclear if we can directly utilize their learned embeddings for phrase grounding without fine-tuning. To this end, we propose a method to extract matched phrase-region pairs from pre-trained vision-and-language embeddings and propose four fine-tuning objectives to improve the model phrase grounding ability using image-caption data without any supervised grounding signals. Experiments on two representative datasets demonstrate the effectiveness of our objectives, outperforming baseline models in both weakly-supervised and supervised phrase grounding settings. In addition, we evaluate the aligned embeddings on several other downstream tasks and show that we can achieve better phrase grounding without sacrificing representation generality."
}

@inproceedings{surikuchi-etal-2023-groovist,
    title = "{GROOV}i{ST}: A Metric for Grounding Objects in Visual Storytelling",
    author = "Surikuchi, Aditya K  and
      Pezzelle, Sandro  and
      Fern{\'a}ndez, Raquel",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.202/",
    doi = "10.18653/v1/2023.emnlp-main.202",
    pages = "3331--3339",
    abstract = "A proper evaluation of stories generated for a sequence of images{---}the task commonly referred to as visual storytelling{---}must consider multiple aspects, such as coherence, grammatical correctness, and visual grounding. In this work, we focus on evaluating the degree of grounding, that is, the extent to which a story is about the entities shown in the images. We analyze current metrics, both designed for this purpose and for general vision-text alignment. Given their observed shortcomings, we propose a novel evaluation tool, GROOViST, that accounts for cross-modal dependencies, \textit{temporal misalignments} (the fact that the order in which entities appear in the story and the image sequence may not match), and human intuitions on visual grounding. An additional advantage of GROOViST is its modular design, where the contribution of each component can be assessed and interpreted individually."
}

@misc{liu2024surveytextguided3dvisual,
      title={A Survey on Text-guided 3D Visual Grounding: Elements, Recent Advances, and Future Directions}, 
      author={Daizong Liu and Yang Liu and Wencan Huang and Wei Hu},
      year={2024},
      eprint={2406.05785},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.05785}, 
}

@misc{chen2020scanrefer3dobjectlocalization,
      title={ScanRefer: 3D Object Localization in RGB-D Scans using Natural Language}, 
      author={Dave Zhenyu Chen and Angel X. Chang and Matthias Nießner},
      year={2020},
      eprint={1912.08830},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1912.08830}, 
}
